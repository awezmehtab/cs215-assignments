\chapter{4}
\begin{task}{Task A}
	We can write
	\begin{equation}
		\mathop{\mathbb{E}}[X]= P\{X<a\}\cdot \mathop{\mathbb{E}}[X|X<a]+P\{X\geq a\}\cdot  \mathop{\mathbb{E}}[X|X\geq a].
	\end{equation}
	where $\mathop{\mathbb{E}}[X|X<a]$ is greater than zero and as random variable $X $ is non negative and $\mathop{\mathbb{E}}[X|X\geq a]$ is greater than or equal to $a$ since conditional expression only accounts for values greater than equal to $a$. Therefore, we can say
	\begin{equation}
		\mathop{\mathbb{E}}[X]\geq P\{X\geq a\}\cdot  \mathop{\mathbb{E}}[X|X\geq a]
	\end{equation}
	which leads to
	\begin{align}
		\mathop{\mathbb{E}}[X] & \geq a\cdot P\{X\geq a\}              \\
		P\{X\geq a\}           & \leq \frac{\mathop{\mathbb{E}}[X]}{a}
	\end{align}

	\textbf{Formal Proof} \\
	For non negative continuous random variable with density $f$, we can write the expected value as
	\begin{align}
		\mathop{\mathbb{E}}[X]          & =\int_{0}^{\infty}xf(x)dx                        \\
		                                & = \int_{0}^{a}xf(x)dx + \int_{a}^{\infty}xf(x)dx \\
		                                & \geq \int_{a}^{\infty}xf(x)dx                    \\
		                                & \geq \int_{a}^{\infty}af(x)dx                    \\
		                                & = a \int_{a}^{\infty}f(x)dx                      \\
		                                & =aP\{X\geq a\}                                   \\
		\implies \mathop{\mathbb{E}}[X] & \geq aP\{X\geq a\}                               \\
		\implies P\{X\geq a\}           & \leq \frac{\mathop{\mathbb{E}}[X]}{a}
	\end{align}
	Hence we arrive at our required relation.
\end{task}


\begin{task}{Task B}
	To prove:
	\begin{equation}
		P\big[X- \mu \geq \tau \big] \leq \frac{\sigma^2}{\sigma^2+\tau^2}
	\end{equation}
	for every $\tau >0$ where $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. \\
	\par Let $Y=X-\mu$. So,we have
	\begin{align}
		P[Y\geq \tau]          & =P[Y+\mu \geq \tau+\mu] \leq P[(Y+\mu)^2\geq (\tau+\mu)^2]                              \\
		\implies P[Y\geq \tau] & \leq \frac{\mathop{\mathbb{E}}[(Y+\mu)^2]}{(\tau+\mu)^2}                                \\
		\implies P[Y\geq \tau] & \leq \frac{\sigma^2+\mu^2}{(\tau+\mu)^2} \:\:\: \text{since $E[(Y+\mu)^2]$ is $E[X^2]$}
	\end{align}
	Since this probability holds for all $\mu$ and $\tau$, we can put $\mu=\frac{\sigma^2}{\tau}$ to minimise the function by differentiating and we have
	\begin{align}
		P[Y\geq \tau]          & \leq \frac{\sigma^{2}(\tau^2+\sigma^2)}{(\sigma^2+\tau^2)^2} \\
		\implies P[Y\geq \tau] & \leq \frac{\sigma^{2}}{\sigma^2+\tau^2}
	\end{align}
	Hence proved.
\end{task}


\begin{task}{Task C}
	To show:
	\begin{align}
		P[X\geq x] & \leq e^{-tx}M_{x}(t) \:\:\forall t>0 \\
		P[X\leq x] & \leq e^{-tx}M_{x}(t) \:\:\forall t<0
	\end{align}
	From markov inequality, we know that for a non negative random variable $X$ and value $x>0$, we can write
	\begin{align}
		P[X\geq x]\leq \frac{E[X]}{x}
	\end{align}
	Using the fact that $e^{tx}$ is always positive, we can write $P(e^{tX}\geq e^{tx})$ same as $P(X>=x)$, we get:
	\begin{align}
		P(X\geq x)\leq \frac{\mathop{\mathbb{E}}[e^{tX}]}{e^{tx}}
	\end{align}
	By definition, we know $\mathop{\mathbb{E}}[e^{tX}]=M_{X}(t)$, therefore
	\begin{equation}
		P(X\geq x)\leq \frac{M_{X}(t)}{e^{tx}}=e^{-tx}M_{X}(t)
	\end{equation}
	For second part, since we need to prove for $t<0$ and we want to bound the probability $P(X\leq x)$, we can rewrite it as
	\begin{equation}
		P(X \leq x) = P(-X\geq -x)
	\end{equation}
	Using markov's inequality on $e^{t(-X)}=e^{-tX}$ (Since $t<0$)
	\begin{equation}
		P(e^{-tX}\geq e^{-tx})\leq \frac{\mathop{\mathbb{E}}[e^{-tX}]}{e^{-tx}}
	\end{equation}
	Now substituting $\mathop{\mathbb{E}}[e^{-tX}]=M_{X}(-t)$ and $- t$ with $t$ since $t<0$ we get
	\begin{equation}
		P(X\leq x)\leq e^{-tx}M_{X}(t)
	\end{equation}
	Hence both proofs are finished under given conditions.
\end{task}
\begin{task}{Task D}
	We are given $Y = \sum^n_{i=1}X_i$, where each $X_i$ is an independent Bernoulli random variable with $\mathds{E}[X_i] = p_i$
	\subsection*{1. Expectation of $Y$}
	The new random variable $Y$ is given by:
	\begin{align}
		Y = \sum^n_{i=1}X_i
	\end{align}
	Using the liniearity of expectation, the expectation of $Y$ is:
	\begin{align}
		\mathds{E}[X_i] = \mathds{E}\left[\sum_{i=1}^{n}\right] = \sum_{i=1}^{n}\mathds{E}[X_i] = \sum_{i=1}^{n}p_i
	\end{align}
	Let $\mu = \sum_{i=1}^{n}p_i$, So $\mathds{E}[Y]=\mu$
	\subsection*{2. Proof}
	From previous subparts, we know
	\begin{align}
		P[X\geq x]\leq e^{-tx}M_X(t) \text{ for all } t>0
	\end{align}
	where $M_X(t)$ is the MGF.
	\begin{align}
		M_Y(t) & = \mathds{E}[e^{tY}]                             \\
		       & = \mathds{E}[e^{t\sum_{i=1}^{n}X_i}]             \\
		       & = \mathds{E}\left[\prod_{i=1}^{n}e^{tX_i}\right]
	\end{align}
	Using the independence of each $X_i$, we can write the expecatation of product as product of expectation,
	\begin{align}
		M_Y(t) & = \prod_{i=1}^{n}\mathds{E}[e^{tX_i}] \\
		       & = \prod_{i=1}^{n}M_{X_i}
	\end{align}
	MGF of a Bernoulli random variable is
	\begin{align}
		M_{X_i} = p_ie^t+(1-p_i)
	\end{align}
	So,
	\begin{align}
		M_Y(t) = \prod_{i=1}^{n}(p_ie^t+(1-p_i))
	\end{align}
	Putting it back into the inequality,
	\begin{align}
		P[X\geq (1+\delta)\mu]\leq \frac{\prod_{i=1}^{n}(p_ie^t+(1-p_i))}{e^{t(1+\delta)\mu}}
	\end{align}
	We know that, $1+x \leq e^x$. Substituting $x$ with $p_i(e^t-1)$, we get:
	\begin{align}
		1+p_i(e^t-1) \leq e^{p_i(e^t-1)}
	\end{align}
	Rearranging terms,
	\begin{align}
		p_ie^t + (1-p_i) \leq e^{p_i(e^t-1)}
	\end{align}
	So,
	\begin{align}
		P[X\geq (1+\delta)\mu] & \leq \frac{\prod_{i=1}^{n}e^{p_i(e^t-1)}}{e^{t(1+\delta)\mu}} \\
		P[X\geq (1+\delta)\mu] & \leq \frac{e^{\sum_{i=1}^{n}p_i(e^t-1)}}{e^{t(1+\delta)\mu}}  \\
		P[X\geq (1+\delta)\mu] & \leq \frac{e^{(e^t-1)\sum_{i=1}^{n}p_i}}{e^{(1+\delta)t\mu}}  \\
		P[X\geq (1+\delta)\mu] & \leq \frac{e^{(e^t-1)\mu}}{e^{(1+\delta)t\mu}}
		\label{en:q4dineq}
	\end{align}
	This completes the proof
	\section*{3. Improving bound}
	To improve bound, we have to minimise the right hand term in \ref{en:q4dineq}. As it is a continuous functions, we can differentiate it to get a minima. Let the RHS be $f(t)$
	\begin{align}
		\diff{f}{t} = \diff{\frac{e^{(e^t-1)\mu}}{e^{(1+\delta)t\mu}}}{t}
	\end{align}
	For minima, $\diff{f}{t} $ should be 0.
	\begin{align}
		\diff{\frac{e^{(e^t-1)\mu}}{e^{(1+\delta)t\mu}}}{t}                                                                       & = 0             \\
		\frac{(e^{(1+\delta)t\mu})\diff{e^{(e^t-1)\mu}}{t}- e^{(e^t-1)\mu}\diff{e^{(1+\delta)t\mu}}{t}}{{(e^{(1+\delta)t\mu})}^2} & = 0             \\
		(e^{(1+\delta)t\mu})(e^{(e^t-1)\mu})(\mu e^t) - (e^{(e^t-1)\mu})((e^{(1+\delta)t\mu}))(1+\delta)\mu                       & = 0             \\
		e^t - (1+\delta)                                                                                                          & = 0             \\
		t                                                                                                                         & = \ln(1+\delta)
	\end{align}
	The improved bound will be,
	\begin{align}
		f(\ln(1+\delta)) = \frac{e^{(e^{\ln(1+\delta)}-1)\mu}}{e^{(1+\delta){\ln(1+\delta)}\mu}}
	\end{align}
	On further calculation, we get,
	\begin{align}
		f(\ln(1+\delta)) = \frac{e^{\delta \mu}}{(1+\delta)^{(1+\delta)\mu}}
	\end{align}
	Finally,
	\begin{align}
		P[X\geq (1+\delta)\mu] \leq \frac{e^{\delta \mu}}{(1+\delta)^{(1+\delta)\mu}}
	\end{align}
    \end{task}


\begin{task}{Task E(B)}
 We need to prove Weak Law of Large Numbers using Chernoff bound, that is, for
 any $\epsilon>0$, the probability that sample average deviates from the true
 mean $\mu$ by more than $\epsilon$ tends to zero as $n\rightarrow \infty$,
 that is
 \begin{equation}
     \lim_{n\to\infty}P(|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu|>\epsilon)=0  
 \end{equation}
 Applying the Chernoff bound, we need to bound $A_{n}$.
 \begin{equation}
     P(|A_{n}-\mu|> \epsilon )=P(A_{n}>\mu+\epsilon)+P(A_{n}<\mu - \epsilon)
 \end{equation}
 Considering the probability $P(A_{n}>\mu+\epsilon)$. The moment generating function for the i.i.d. random variable $X_{i}$ as $M_{X}(t)=\mathop{\mathbb{E}}[e^{tX_{i}}]$. Using Chernoff's bound on this probability,keeping $\epsilon=\delta\mu$ we get
 \begin{equation}
     \lim_{n\to\infty}P(A_{n}>\mu+\epsilon)< \inf_{t>0}(e^{n[(e^{t}-1)\mu - (1+\delta)t\mu]})
 \end{equation}
 Putting the value of t from the earlier task we get,
 \begin{equation}      
    \lim_{n\to\infty}P(A_{n}>\mu+\epsilon)<\lim_{n\to\infty}e^{nu[\delta-(1+\delta)ln(1+\delta)]}
 \end{equation}
 For $t>0$, $\delta $ must be greater than 1 and hence the exponential power becomes negative and thus as n grow larger the term tends to zero.
 \par For the other part, i.e., $P(A_{n}<\mu - \epsilon)$ using the chernoff bound on the left tail and putting $\epsilon=\delta\mu$,
 \begin{equation}
     \lim_{n\to\infty}P(A_{n}<\mu - \epsilon)< \inf_{t<0}(e^{n[(e^{t}-1)\mu - (1 -\delta)t\mu]})
 \end{equation}
 Since minimising while $t<0$ we will be using $-1<\delta<0$ and putting value of t, we get
 \begin{equation}
     \lim_{n\to\infty}P(A_{n}<\mu - \epsilon)<(e^{nu(\delta-(1-\delta) ln(1+\delta))})
 \end{equation}
 Again, we can see that the exponential power becomes negative and therefore as n rises, the probability tends to zero.
 \par Therefore we can conclude as n tends to infinity, the probability tends to zero from both sides.
\end{task}
